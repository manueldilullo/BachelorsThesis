\subsection{Primi approcci}
La storia della NLP viene fatta spesso partire dagli anni '50 del XX secolo, quando, nel famoso articolo \textit{Computing Machinery and Intelligence}\textsuperscript{\cite{10.1093/mind/LIX.236.433}}, Alan Turing propose il suo famoso test per valutare l'abilità di un computer di impersonare un umano durante una conversazione scritta in tempo reale. In realtà, quasi un decennio prima dell'uscita del suddetto articolo, si parlava già del concetto di \textit{Machine Translation}, un sottocampo della linguistica computazionale che studia l'uso di software per tradurre testi o discorsi da una lingua all'altra. Ma ciò è stato solo l'inizio della lunga storia dell'elaborazione del linguaggio naturale. Difatti, con il passare degli anni, si sono succeduti diversi approcci alla NLP. 

Nel 1957, il linguista americano Noam Chomsky, pubblicò \textit{Syntactic Structures}\textsuperscript{\cite{chomsky2002syntactic}}, opera che offrì un contributo fondamentale al problema introducendo la \textit{grammatica generativa}, insieme di regole che specificano in modo formale e ricorsivo le strutture sintattiche di un linguaggio. Ad esso, fino agli anni '80, anche a causa di un congelamento dei fondi USA destinati alla ricerca, seguirono sistemi NLP costituiti da strutture estremamente complesse di regole procedurali ed euristiche, che però a causa della loro rigidità risultarono incapaci di gestire l'estrema variabilità ed ambiguità del linguaggio naturale.

A cavallo tra gli anni '80 e '90, c'è stata una vera e propria rivoluzione dettata dall'introduzione degli algoritmi di machine learning per l'elaborazione del linguaggio. Si passò da sistemi \textit{rule-based} codificati manualmente a sistemi \textit{corpus-based} nei quali l'intervento umano diretto veniva limitato grazie all'apprendimento automatico a partire da uno o più corpus di riferimento. La maggior parte di questi approcci, utilizzati anche attualmente, forniscono un modo più avanzato per interpretare l'ambiguità e fornire ulteriori prove per la valutazione di una decisione. Algoritmi come gli alberi decisionali usano regole \textit{if-then} per ottenere il risultato ottimale e algoritmi probabilistici che sostengono la decisione presa dalla macchina fornendo una buona accuratezza.

Nel 2011, per la prima volta un algoritmo basato sul deep learning è stato applicato a differenti problemi di NLP, tra cui l’identificazione di entità e l’assegnazione di categorie morfologiche a parole, mostrando prestazioni sensibilmente migliori rispetto ad altri approcci rappresentativi dello stato dell’arte. Da allora, sono stati realizzati algoritmi sempre più complessi basati sul deep learning per affrontare problemi di NLP ancora non risolti o trattati in passato ma con risultati non soddisfacenti. 

Il Deep Learning è un ramo del Machine Learning, nel quale viene utilizzata una rete neurale con tre o più strati. Queste reti neurali tentano di simulare il comportamento del cervello umano permettendogli di "imparare" da grandi quantità di dati. Mentre una rete neurale con un solo strato può ancora fare previsioni approssimative, ulteriori strati nascosti possono aiutare a ottimizzare e raffinare la precisione.