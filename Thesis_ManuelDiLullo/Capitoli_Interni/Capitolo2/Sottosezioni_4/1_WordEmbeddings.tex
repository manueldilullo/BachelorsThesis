\subsection{Word Embedding}
Word embedding è il nome collettivo per definire un insieme di tecniche di language modeling e di apprendimento delle caratteristiche nell'elaborazione del linguaggio naturale in cui le parole o le frasi del vocabolario sono mappate in vettori di numeri reali. La necessità di questa pratica è nata con lo sviluppo delle reti neurali artificiali dato che, come accennato nella sezione riguardante il Deep Learning, esse possono elaborare soltanto input di natura numerica.

L'embedding si basa sul fatto che, in genere, le parole con un significato simile avranno rappresentazioni vettoriali che sono vicine tra loro nello spazio di incorporazione (anche se questo non è sempre stato il caso). Quando si codificano le parole, tipicamente l'obiettivo è quello di catturare qualche tipo di relazione in quello spazio, che sia il significato, la morfologia, il contesto o qualche altro tipo di connessione.

Molti word embedding sono creati sulla base della nozione introdotta dall'\textit{ipotesi distributiva} di Zellig Harris\textsuperscript{\cite{distributional_structure}}, che si riduce a una semplice idea che le parole che sono usate vicine l'una all'altra hanno tipicamente lo stesso significato.

Ciò diventa particolarmente utile quando i set di dati diventano sempre più grandi, perché con l’aumentare delle dimensioni spesso aumenta anche il numero di parole uniche. La presenza di molte parole usate raramente può causare problemi per un modello lineare; questo perché la quantità di possibili sequenze di parole aumenta, e i modelli che informano i risultati diventano più deboli. Ponderando le parole in modo non lineare e distribuito, questo modello può "imparare" ad approssimare le parole e quindi non essere fuorviato da eventuali valori sconosciuti. La sua "comprensione" di una data parola non è così strettamente legata alle parole immediatamente circostanti.
