\subsection{Embedding contestualizzati: Universal Sentence Encoder}
BERT è uno strumento molto potente ma, proprio per questo motivo non è la soluzione migliore quando si tratta di utilizzarlo su dispositivi con memoria o potenza di calcolo limitata. È da questa idea che nasce il progetto proposto da \citet{cer2018universal}.

L'\textbf{Universal Sentence Encoder (USE)} codifica il testo in vettori ad alta dimensione che possono essere utilizzati per la classificazione del testo, la similarità semantica, il clustering e altri compiti del Natural Language Processing. L'idea è quella di progettare un codificatore che riassuma qualsiasi frase data in un'embedding a 512 dimensioni. Si usa questo stesso embedding per risolvere compiti multipli e in base agli errori che vengono fatti su di essi, aggiorniamo la codifica della frase. Poiché essa deve lavorare su più compiti generici, catturerà solo le caratteristiche più informative e scarterà il rumore. L'intuizione è che, così facendo, il risultato possa essere universalmente compatibile per essere incorporato a diversi task anche molto distanti tra loro.

L'Universal Sentence Encoder pre-addestrato è disponibile pubblicamente in \textbf{Tensorflow-hub}. Viene fornito in due varianti:
\begin{enumerate}
    \item \textbf{Transformer Encoder}: In questa variante, usiamo la parte encoder dell'architettura originale del transformer. L'architettura consiste di 6 strati di trasformatori impilati. Ogni strato ha un modulo di self-attention seguito da una rete feed-forward.
    \item \textbf{Deep Averaging Network}: Word embeddings e i bi-grammi presenti in una frase sono mediate insieme. Poi attraversano una DNN profonda a 4 strati feed-forward per ottenere in uscita un'embedding per l'intera frase a 512 dimensioni. Le embeddings per le parole e i bi-grammi sono apprese durante l'addestramento.
\end{enumerate}
I due modelli hanno un compromesso di accuratezza e richiesta di risorse computazionali. Mentre quello con un codificatore Transformer ha una maggiore precisione ma è computazionalmente più intenso, quello con codifica DAN usa meno memoria con un leggero compromesso a livello di precisione.

Il modello di codifica è progettato per essere il più generale possibile. Ciò viene realizzato utilizzando l'apprendimento multi-task in cui un singolo modello di codifica viene addestrato per risolvere più compiti diversi. I tasks supportati includono: un compito simile a Skip-Thought (\citet{NIPS2015_f442d33f}) per l'apprendimento non supervisionato da un testo corrente arbitrario; un task di input-risposta conversazionale (\citet{henderson2017efficient}); e compiti di classificazione per l'addestramento su dati supervisionati. Il compito Skip-Thought sostituisce il LSTM (\citet{Hochreiter1997LongSM}) usato nella formulazione originale con un modello un modello basato sull'architettura Transformer. 